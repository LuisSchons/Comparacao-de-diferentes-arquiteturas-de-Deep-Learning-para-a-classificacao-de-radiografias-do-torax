{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b778c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lê um arquivo csv com a descrição do dataset e separa em 3, pra treinamento, validação e teste\n",
    "\n",
    "import csv\n",
    "from random import shuffle,seed\n",
    "\n",
    "genlist = []\n",
    "\n",
    "with open(\"dataset.csv\", \"r\") as f:\n",
    "\treader = csv.reader(f)\n",
    "\ti = 0\n",
    "\tfor line in reader:\n",
    "\t\tif i > 0:\n",
    "\t\t\tgenlist.append(line) \n",
    "\t\telse:\n",
    "\t\t\theader = line\n",
    "\t\ti+=1\n",
    "\n",
    "f.close()\n",
    "\n",
    "seed()\n",
    "for i in range(4):\n",
    "\tshuffle(genlist)\n",
    "size = len(genlist)\n",
    "i = 0\n",
    "\n",
    "markings = open('test.csv','w', encoding=\"utf-8\")\n",
    "writer = csv.writer(markings)\n",
    "writer.writerow(header)\n",
    "\n",
    "j = 0\n",
    "while j < int(0.1*size):\t\t\t\t\t#10% para teste\n",
    "\twriter.writerow(genlist[i])\n",
    "\ti+=1\n",
    "\tj+=1\n",
    "\n",
    "markings.close()\n",
    "\n",
    "markings = open('validation.csv','w', encoding=\"utf-8\")\n",
    "writer = csv.writer(markings)\n",
    "writer.writerow(header)\n",
    "\n",
    "j = 0\n",
    "while j < int(0.1*size):\t\t\t\t\t#10% para validacao\n",
    "\twriter.writerow(genlist[i])\n",
    "\ti+=1\n",
    "\tj+=1\n",
    "\n",
    "markings.close()\n",
    "\n",
    "markings = open('train.csv','w', encoding=\"utf-8\")\n",
    "writer = csv.writer(markings)\n",
    "writer.writerow(header)\n",
    "\n",
    "while i < size:\n",
    "\twriter.writerow(genlist[i])\t\t\t\t#80% para treino\n",
    "\ti+=1\n",
    "\n",
    "markings.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f432d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Treina uma CNN, possui várias opções de backbone pra usar. Recomendo começar pela MobileNet. Usa os arquivos CSV criados anteriormente\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from  keras.layers import Input\n",
    "import pandas as pd\n",
    "from math import log\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "from tensorflow.keras.regularizers import l1,l2\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"0\"\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "assert len(physical_devices) > 0, 'Not enough GPU hardware devices available'\n",
    "\n",
    "train_datagen = ImageDataGenerator()\n",
    "test_datagen = ImageDataGenerator()\n",
    "\n",
    "train_annot = pd.read_csv('train.csv',header=0)\n",
    "y_train = train_annot['Findings']\n",
    "test_annot = pd.read_csv('validation.csv',header=0)\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(dataframe=train_annot,directory='dataset', class_mode='raw', x_col='Image Index', y_col=\"Findings\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\ttarget_size=(320,320), batch_size=16, color_mode='grayscale')\n",
    "\n",
    "#color_mode='grayscale'\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(dataframe=test_annot,directory='dataset', class_mode='raw', x_col='Image Index', y_col='Findings',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\ttarget_size=(320, 320), batch_size=16, color_mode='grayscale')\n",
    "\n",
    "\n",
    "inn = layers.Input(shape=(320,320,1))\n",
    "# x = layers.experimental.preprocessing.Rescaling(1./255.)(inn)\n",
    "\n",
    "VGG16 = tf.keras.applications.vgg16.VGG16(\n",
    "\tinclude_top=False, weights= None, input_tensor=inn,\n",
    ")\n",
    "backbone = VGG16\n",
    "\n",
    "#MOBILENET = tf.keras.applications.mobilenet.MobileNet(\n",
    "#\tinclude_top=False, weights='imagenet', input_tensor=inn,\n",
    "#)\n",
    "#backbone = MOBILENET\n",
    "\n",
    "#MOBILENETv2 = tf.keras.applications.mobilenet_v2.MobileNetV2(\n",
    "#\tinclude_top=False, weights='imagenet', input_tensor=inn,\n",
    "#)\n",
    "#backbone = MOBILENETv2\n",
    "\n",
    "#MOBILENETv3 = tf.keras.applications.MobileNetV3Small(\n",
    "#\tinclude_top=False, weights='imagenet', input_tensor=inn,\n",
    "#)\n",
    "#backbone = MOBILENETv3\n",
    "\n",
    "#DENSENET = tf.keras.applications.densenet.DenseNet121(\n",
    "#\tinclude_top=False, weights= None, input_tensor=inn,\n",
    "#)\n",
    "#backbone = DENSENET\n",
    "\n",
    "#RESNET50 = tf.keras.applications.resnet50.ResNet50(\n",
    "#    include_top=False, weights='imagenet',input_tensor=inn\n",
    "#)\n",
    "#backbone = RESNET50\n",
    "\n",
    "#RESNET101 = tf.keras.applications.resnet.ResNet101(\n",
    "#    include_top=False, weights='imagenet',input_tensor=inn\n",
    "#)\n",
    "#backbone = RESNET101\n",
    "\n",
    "# NasNetMobile = tf.keras.applications.nasnet.NASNetMobile(\n",
    "#     include_top=False, weights='imagenet',input_tensor=inn,\n",
    "# )\n",
    "# backbone = NasNetMobile\n",
    "\n",
    "#EFFICIENTNET = tf.keras.applications.efficientnet_v2.EfficientNetV2B0(\n",
    "#    include_top=False, weights='imagenet',input_tensor=inn,\n",
    "#)\n",
    "#backbone = EFFICIENTNET\n",
    "\n",
    "#xception = tf.keras.applications.xception.Xception(\n",
    "#    include_top=False, weights='imagenet',input_tensor=inn,\n",
    "#)\n",
    "#backbone = xception    \n",
    "\n",
    "\n",
    "#backbone.trainable = False\n",
    "backbone.trainable = True\n",
    "\n",
    "#x = layers.Conv2D(96, (11, 11), activation='relu')(inn) \t#Adiciona camada de convoluçao\n",
    "#x = layers.MaxPooling2D((2, 2))(x)\t\t\t\t\t\t\t#Adiciona a camada de pooling\n",
    "#x = layers.Conv2D(256, (5, 5), activation='relu')(x)\n",
    "#x = layers.MaxPooling2D((2, 2))(x)\n",
    "#x = layers.Conv2D(384, (3, 3), activation='relu')(x)\n",
    "#x = layers.MaxPooling2D((2, 2))(x)\n",
    "#x = layers.Flatten()(x)\t\t\t\t\t\t\t\t\t\t#Converte a estrutura de dados 2D da camada anterior em uma estrutura 1D, vetor\n",
    "x = layers.Flatten()(backbone.layers[-1].output)\n",
    "#x = layers.BatchNormalization()(x)\n",
    "#x = layers.Dropout(0.1)(x)\n",
    "# x = layers.Dense(1024,activation='linear',kernel_regularizer=l2(0.01))(x)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "#x = layers.Dense(1024,activation='linear',kernel_regularizer=l2(0.01))(x)\n",
    "#x = layers.BatchNormalization()(x)\n",
    "#x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(128,activation='linear',kernel_regularizer=l1(0.01))(x)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "# x = layers.Dropout(0.3)(x)\n",
    "o = layers.Dense(10,activation='softmax',kernel_regularizer=l1(0.01))(x)\n",
    "\n",
    "#------------ ALEXNET -----------------\n",
    "\n",
    "# 1st Convolutional Layer\n",
    "#x = layers.Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', weights=None)(inn)\n",
    "#x = layers.MaxPooling2D(pool_size=(3,3), strides=(2,2))(x)\n",
    "#x = layers.BatchNormalization()(x)\n",
    "\n",
    "# 2nd Convolutional Layer\n",
    "#x = layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), padding=\"same\", activation='relu', weights=None)(x)\n",
    "#x = layers.MaxPooling2D(pool_size=(3,3), strides=(2,2))(x)\n",
    "#x = layers.BatchNormalization()(x)\n",
    "\n",
    "# 3rd Convolutional Layer\n",
    "#x = layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding=\"same\", activation='relu', weights=None)(x)\n",
    "#x = layers.BatchNormalization()(x)\n",
    "\n",
    "# 4th Convolutional Layer\n",
    "#x = layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding=\"same\", activation='relu', weights=None)(x)\n",
    "#x = layers.BatchNormalization()(x)\n",
    "\n",
    "# 5th Convolutional Layer\n",
    "#x = layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding=\"same\", activation='relu', weights=None)(x)\n",
    "#x = layers.MaxPooling2D(pool_size=(3,3), strides=(2,2))(x)\n",
    "#x = layers.BatchNormalization()(x)\n",
    "\n",
    "# Flatten Layer\n",
    "#x = layers.Flatten()(x)\n",
    "\n",
    "# 1st Dense Layer\n",
    "#x = layers.Dense(units=4096, activation='relu')(x)\n",
    "#x = layers.Dropout(rate=0.5)(x)\n",
    "#x = layers.BatchNormalization()(x)\n",
    "\n",
    "# 2nd Dense Layer\n",
    "#x = layers.Dense(units=4096, activation='relu')(x)\n",
    "#x = layers.Dropout(rate=0.5)(x)\n",
    "#x = layers.BatchNormalization()(x)\n",
    "\n",
    "# 3rd Dense Layer\n",
    "#x = layers.Dense(units=128, activation='relu')(x)\n",
    "#x = layers.Dropout(rate=0.5)(x)\n",
    "#x = layers.BatchNormalization()(x)\n",
    "\n",
    "# Output Layer\n",
    "#o = layers.Dense(units=10, activation='softmax')(x)\n",
    "\n",
    "\n",
    "class_weights = compute_class_weight('balanced',classes=np.unique(y_train),y=y_train)\n",
    "# for i in range(len(class_weights)):\n",
    "#\tclass_weights[i] = class_weights[i]**0.5\n",
    "# \tclass_weights[i] = min(class_weights[i],20)\n",
    "# \t# class_weights[i] = max(class_weights[i],0.6)\n",
    "# class_weights = np.insert(class_weights,3,0.0)\n",
    "# class_weights[0] = .4\n",
    "# class_weights[1] = 1.2\n",
    "# class_weights[2] = 2.4\n",
    "# class_weights[3] = 3.6\n",
    "# class_weights[4] = 4.8\n",
    "# class_weights[5] = 6.0\n",
    "# class_weights[6] = 7\n",
    "# class_weights[7] = 7\n",
    "# class_weights[8] = 7\n",
    "# class_weights[9] = 8\n",
    "print(class_weights)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "model = tf.keras.Model(inputs=inn,outputs=[o])\n",
    "\n",
    "\n",
    "# model.summary()\n",
    "# model.load_weights('cnn_model.h5')\n",
    "\n",
    "num_epochs = 400\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\t\t\t#Usa o otimizador Adam para compilar os dados padrao 10^-3\n",
    "model.compile(optimizer=optimizer,\t\t\t\t\t\t\t\t\t#Complila a rede\n",
    "\t\t\t\tloss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "\t\t\t\tmetrics = ['accuracy'])\n",
    "\n",
    "reduce_on_plateau = tf.keras.callbacks.ReduceLROnPlateau(\t\t\t#Reduz learning rate baseado na accuracy dadas x epocas\n",
    "        monitor  = 'val_loss',\n",
    "        factor   = 0.1,\n",
    "        patience = 5,\n",
    "        verbose  = 0,\n",
    "        mode     = 'min',\n",
    "        min_delta  = 0.001,\n",
    "        cooldown = 0,\n",
    "        min_lr   = 0\n",
    "    )\n",
    "\n",
    "history = model.fit(train_generator,\n",
    "\t\tvalidation_data=test_generator,\n",
    "\t\tsteps_per_epoch=128,\n",
    "\t\tepochs = num_epochs,\n",
    "\t\t# validation_steps = 64,\n",
    "\t\tclass_weight=class_weight_dict,\n",
    "\t\tverbose = 1,\n",
    "\t\tcallbacks=[\n",
    "\t\t# tf.keras.callbacks.LearningRateScheduler(\n",
    "\t\t# \tlambda epoch: 0.0001/((epoch+1)**0.5)\n",
    "\t\t# ),\n",
    "\t\ttf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=15,mode = 'min', min_delta=0.001,restore_best_weights=True),\n",
    "\t\treduce_on_plateau\n",
    "\t])\n",
    "\n",
    "model.save('cnn_model.h5')\n",
    "\n",
    "model.evaluate(test_generator)\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(len(acc))\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss') \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab98688a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
